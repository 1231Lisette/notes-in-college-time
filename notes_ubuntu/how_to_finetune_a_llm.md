æ•°æ®å‡†å¤‡

```python
class textdataset(dataset):
    def __init__(self, file_path,tokenizer,block_size=512):
        # è¯»å–æ–‡ä»¶
        # æŒ‰ç…§æŸç§è§„åˆ™è¿›è¡Œåˆ†å‰²
        # å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç 
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self,idx):
			return self.examples[idx]
        
def get_batch(tokenizer, train_file, val_file=None,block_size):
    # å‡†å¤‡è®­ç»ƒé›†å’ŒéªŒè¯é›†
    return train_dataset, val_dataset
```

---

æ¨¡å‹åŠ è½½å’Œé…ç½®

```python
def setup_model_and_tokenizer(model_name="gpt2", output_dir="./fine-tuned-model):
	# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
	return model, tokenizer, output_dir
```

---

è®­ç»ƒå‚æ•°è®¾ç½®

```python
def get_training_args(output_dir, btach_size=4, grad_accum=4):
	# è®¾ç½®è®­ç»ƒå‚æ•°pythonpython
    effective_batch_size = batch_size * grad_accum
    
    training_args = TrainingArguments(
        output_dir=output_dir,
        overwrite_output_dir=True,
        num_train_epochs=3,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        gradient_accumulation_steps=grad_accum,
        warmup_steps=100,
        logging_steps=10,
        save_steps=500,
        eval_steps=500,
        evaluation_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        learning_rate=5e-5,
        weight_decay=0.01,
        fp16=torch.cuda.is_available(),
        dataloader_pin_memory=False,
        report_to=None,  # ç¦ç”¨wandbç­‰è®°å½•
    )
    
    return training_args
```

---

å®Œæ•´è®­ç»ƒæµç¨‹

```python
def fine_tune_model(train_file, val_file=None, model_name="gpt2"):
    """å®Œæ•´çš„å¾®è°ƒæµç¨‹"""
    
    # 1. è®¾ç½®æ¨¡å‹å’Œtokenizer
    print("ğŸ”„ åŠ è½½æ¨¡å‹å’Œtokenizer...")
    model, tokenizer, output_dir = setup_model_and_tokenizer(model_name)
    
    # 2. å‡†å¤‡æ•°æ®
    print("ğŸ“š å‡†å¤‡æ•°æ®...")
    train_dataset, val_dataset = prepare_data(tokenizerpython, train_file, val_file)
    
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}")
    print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_dataset)}")
    
    # 3. è®¾ç½®æ•°æ®æ”¶é›†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # å¯¹äºGPTæ˜¯å› æœè¯­è¨€æ¨¡å‹ï¼Œä¸æ˜¯æ©ç è¯­è¨€æ¨¡å‹
    )
    
    # 4. è®¾ç½®è®­ç»ƒå‚æ•°
    training_args = get_training_args(output_dir)
    
    # 5. åˆ›å»ºTrainer
    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
    )
    
    # 6. å¼€å§‹è®­ç»ƒ
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    trainer.train()
    
    # 7. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
    trainer.save_model()
    tokenizer.save_pretrained(output_dir)
    
    print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")
    
    return model, tokenizer
    python
```

---

æ¨ç†å’Œä½¿ç”¨

```python
def generate_text(model, tokenizer, prompt, max_length=100):
    """ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆæ–‡æœ¬"""
    
    inputs = tokenizer(prompt, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model.generate(
            inputs.input_ids,
            max_length=max_length,
            num_return_sequences=1,
            temperature=0.8,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_text

def load_fine_tuned_model(model_path):
    """åŠ è½½å¾®è°ƒåçš„æ¨¡å‹"""
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path)
    return model, tokenizer
```

---

ä¸»å‡½æ•°

```python
def main():
    """ä¸»å‡½æ•°"""
    
    # é…ç½®å‚æ•°
    config = {
        "train_file": "your_data.txt",  # ä½ çš„txtæ–‡ä»¶è·¯å¾„
        "val_file": None,               # éªŒè¯é›†æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        "model_name": "gpt2",           # é¢„è®­ç»ƒæ¨¡å‹åç§°
        "output_dir": "./my-fine-tuned-model",
    }
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(config["train_file"]):
        print(f"âŒ è®­ç»ƒæ–‡ä»¶ä¸å­˜åœ¨: {config['train_file']}")
        return
    
    try:
        # æ‰§è¡Œå¾®è°ƒ
        model, tokenizer = fine_tune_model(
            train_file=config["train_file"],
            val_file=config["val_file"],
            model_name=config["model_name"]
        )
        
        # æµ‹è¯•ç”Ÿæˆ
        print("\nğŸ§ª æµ‹è¯•ç”Ÿæˆ...")
        test_prompt = "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œ"
        generated = generate_text(model, tokenizer, test_prompt)
        print(f"è¾“å…¥: {test_prompt}")
        print(f"ç”Ÿæˆ: {generated}")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
```

