#  一、NLP / CV 的自回归模型（Autoregressive Models）

###  1. 自回归的核心思想

**一句话：下一个 token 由之前所有 token 决定。**

公式：

> p(x) = Π p(x_t | x < t)

无论 NLP 字符串还是 CV 图像 token，都是用这个逻辑。

### 2. NLP 里的自回归模型

**发展序列：RNN → LSTM → Transformer → GPT 系列**

- **Transformer Decoder** 结构天生就是自回归（masked attention）
- GPT 就是纯自回归语言模型
- 输入上下文 → 预测下一个 token → 不断生成 → 形成语言序列

特点：

- 强于长序列推理（上下文依赖）
- 序列生成稳定
- 但采样容易积累误差

###  3. CV 中的自回归模型

CV 图像可以被：

- 划成1D sequence（PixelRNN, PixelCNN）
- 或 token 化（VQ-VAE → VQ-GAN → transformer）

例如 ImageGPT：

- 把 32×32 图像 flatten → 1024 tokens → GPT 预测下一个像素 token

缺点：

- 太慢，因为要一次一个 token 地生成整张图。

------

# 二、生成模型从自回归走向扩散模型（Diffusion Models）

### 1. 为什么需要扩散？

自回归在 CV 太慢了：

- 图像 = 10k 级 token
- 逐个预测非常慢

研究者发现：

- **parallel** 生成比 **autoregressive** 更快
- 于是扩散模型火了。

### 2. 扩散模型（DDPM）的思想

核心逻辑：

1. **前向过程**：给图片加噪声 → 最终变成纯噪声
2. **反向过程**：学着从噪声一步步去噪生成图片

优点：

- 并行生成局部像素，不需要 autoregressive token-by-token
- 质量极高（比 GAN 稳定）

缺点：

- 反推要很多步（几十～几百步）

------

#  三、Flow Matching（流匹配）——扩散模型的下一代

### 📌 1. 为什么需要 Flow Matching？

Diffusion 的问题：

- 去噪要太多步（slow）

FM 试图直接学：

> 从噪声 → 数据 的 *可微分路径*
>  从而：

- 不再需要 discrete diffusion steps
- 生成速度更快（几步）

### 📌 2. Flow Matching 核心

而不是学 noise schedule，FM 学的是 **流场 (vector field)**：

> dx/dt = v(x, t)

这样，生成轨迹是连贯的 ODE：

> x_1 = ∫ v dt，从噪声积分到数据点

优点：

- 更快
- 更稳定
- 对连续控制（robotics）更自然

------

# 四、模仿学习（Imitation Learning, IL）走向 ACT / Diffusion Policy

现在进入“具身智能关线”：
 你一直问的 ACT / DP / Flow Policy，就是建立在生成模型之后的 IL 演化。

------

# 4.1 ACT（Action Chunking with Transformers）

### 核心想法：

**机器人动作不是逐步预测，而是成段预测（chunks）**

比如：

- 人类示范是一大堆帧
- ACT 把动作分块
- 每块用 Transformer 编码
- 输出固定长度的动作 chunk（比如 10 帧）

优势：

- 改变了 “one-step-action → propagation → accumulate error” 的老问题
- 更稳，更长时序

缺点：

- chunk有时不够灵活
- 无法捕捉复杂的连续动作分布

------

# 4.2 Diffusion Policy（DP）

DP 把 **扩散模型** 用在 IL 上：

- 输入观测（图像/状态）
- 输出连续动作分布
- 通过 diffusion 去噪一步步得到动作序列

为什么这样更强？

- 动作是连续分布，多模态（同一个状态有多种合理动作）
- 扩散模型可以天然处理 multimodal distribution
- 在 manipulation tasks 中特别有效

DP 的优点：

- 出的动作更 smooth、更 realistic
- 非常稳健
- 已成为当前机器人 manipulation 的主流 baseline

------

# 4.3 Flow Matching for Control / FlowPolicy

FlowPolicy（2024-2025 最火）

- 把 **Flow Matching** 引入机器人动作生成
- 动作生成只需要几步（比DP快很多）
- 控制轨迹更平滑更可控

优点：

- 几乎是 DP 的升级版
- 更适合实时控制（例如机械臂 50~100Hz 控制）

------

# 五、从 IL → VLA（Vision-Language-Action）

这是你现在完全进入的阶段。

### VLA 的本质：

> 把视觉（V）、语言（L）、动作（A）统一到同一个模型里。

未来机器人 = VLA
 不再是：

- 输入图像 → 单独 policy
- 输入指令 → 单独 planner

而是三者统一。

### VLA = LLM + CV encoder + Action Generator

发展路径如下：

------

## 5.1 VLM（Vision-Language Models）

CLIP → BLIP → Flamingo → LLaVA
 这些只能理解视觉，不会做 action。

------

## 5.2 VLA 的诞生：

近两年（2023-2025）爆发：

### 1. RT-1 → RT-2

Google 的路线：

- 用 transformer 让机器人学 vision + action tokens
- RT-2 引入 vision-language 的语义对齐

------

### 2. OpenVLA

完全开源的大模型

- V encoder: SigLIP / DinoV2
- LLM: Llama
- A: Diffusion/Flow-based action解码器

特点：

- 多机器人、多任务的通用性
- 是现在最流行的 baseline

------

###  3. Pi0

DeepMind 2024 的重大突破

- 统一世界模型 + 行动
- 自回归生成动作
- 通过 tokenization 让机器人控制离散化

优点：

- 与大模型 pipeline 完全兼容
- 直接 reasoning → action

你看过的论文里都在吹 Pi0，就是这个原因。

------

###  4. SmolVLA / VLA-mini

小型化、轻量化、成本低
 已经可以在树莓派、Jetson 上跑

------

#  六、总结 —— 这条技术路线是怎么连起来的？

**最清晰的总结**：

1. **NLP 的自回归模型（Transformer → GPT）**
    → 教会我们用序列预测表示复杂结构。
2. **CV 的自回归（ImageGPT）**
    → 证明图像也能用 token 表示。
3. **GAN → Diffusion → Flow Matching**
    → 解决高质量生成（图像、动作、轨迹）的分布建模问题。
4. **Imitation Learning（BC） → ACT → Diffusion Policy → Flow Policy**
    → 生成模型作用到机器人动作生成。
5. **VLA（Vision-Language-Action）**
    → 统一视觉、语言、控制
    → 从 perception → reasoning → action 全链路一体化
    → 实现通用机器人智能