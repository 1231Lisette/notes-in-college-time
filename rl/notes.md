# RL

## 结构和元素

### 第一层结构：基本元素

- goal
- agent
- environment

### 第二层结构：主要元素

- state
- action
- reward

在一个状态下要采取某种行动

奖励是一个即时的反馈，目标是一个长远的结果

### 第三层结构：核心元素

- policy
- value

数学上策略就是一个函数，自变量输入是一个状态，因变量输入是一个行动

价值也是一个函数，策略取决于价值函数。指的是预期将来会得到的所有奖励之和（期望值）

1. 状态价值函数**State value**：输入是一个状态，输出是一个实数
2. 状态行动价值函数**State-Action value**：在特定状态下，采取某种行动所具有的价值

---

> 强化学习是Agent在与环境的互动中为了达成一个目标而进行的学习过程。强化学习所要学习的东西，实际上就是一个好的价值函数，而一个好的价值函数决定一个好的策略

![image1](images/image1.png)

---

## 实例——围棋

agent：我

environment：棋盘和对手

goal：赢得这局棋

state：棋盘上棋子的分布情况

state1：棋盘上没有落子的状态Empty board

action1：在星位落子

reward1：0

state2：对手落子

如果选取直接一点的状态行动价值函数

policy：在state1，我应该在哪里落子呢

state-action value：但是在state1，我可以采取的行动有361种，而这361种都有对应的价值；

如果规定赢棋的奖励为1，输棋或和棋的奖励为0，那么价值其实就是赢棋的概率

所以我的策略就是选择赢棋的概率最大的行动

---

> 核心内容：那么如何去学习一个好的价值函数以及好的策略？

### 两个特点

1. **Trial and Error试错**

强化学习是一种试错学习。

棋谱书告诉你在什么情况下应该怎么落子，是一种监督学习。而棋谱告诉我们的就是一种策略，策略怎么来的，就是围棋的先辈们通过不断的尝试、不断的对弈总结出来的，这就是一种强化学习。

在不断下棋的过程中，去学习哪一步棋的价值最大，在不同的情况下应该怎么落子。

2. **Delayed reward 延迟奖励**

玩家采取的行动得到的即时奖励基本都是0？直到最后获得胜利。也就是行动没有获得即时的奖励，但是每一步棋对于最后的胜利都是有贡献的。这可能就导致一个行动有奖励（但是可能是0），但是它一定有价值。一个行动所具有的价值，只有在得到奖励之后才能真正得到体现，而这个奖励可能发生在一段时间之后。

> 在实际得到奖励之后，我们知道以前所采取的行动都对这个奖励有所贡献，如何学习过去的行动所具有的价值就谁到一个**信用分配**和**反向传播**的过程了
>
> 其实就是复盘，学习每一个行动所具有的价值的过程

---

### 核心问题Expolration(探索) vs Expolitation(利用)

利用强化学习所学习到的价值函数，比如我们有了一个状态行动价值函数，于是应该采取价值最高的行动——>expoitation

但是学习到的价值函数不一定是最优的价值函数，有些看起来不是价值很高的函数，有可能真实的价值是最高的（陷入局部最优解）

这样，我们还应该尝试不同的行动，从而优化我们的价值函数

**如何权衡这俩是核心问题**

---

## 多臂老虎机

- goal：最大化奖励
- agent：我
- environment：两个老虎机
- state只有一个，老虎机摆在不那里不会改变

玩家只需要在不同的行动中选一个

没有延迟奖励问题，因为行动得到的奖励是即时的，不会对以后发生的事情产生影响

可以认为奖励服从一定的概率分布的随机变量，两个老虎机对应的概率分布可能是不同的，但是概率分布是固定的，不会变的

概率分布有一个期望值，最优的选择就应该是期望值更大的那一个 

只有学习state action value状态行动价值，状态没变化，所以就是行动价值

---

我们可以定义：一个行动具有的价值为对应奖励的期望值

左边老虎机奖励$q(L)$，服从$N(500,50)$

右边老虎机奖励$q(R)$，服从$N(550,100)$

我们定义行动价值的估计值为$Q(L)和Q(R)$

如果让他们的初始值为0,我们要怎么也去学习这两个行动价值呢？

尝试：利用实际获得的奖励来估计价值

最简单的方法就是用奖励的平均值作为期望值的估计

于是我们可以写出**行动价值的估计表达式：**在t时刻之前，选择某个行动所实际获得的奖励之和除以这个行动的次数
$$
Q_t(a)=\frac{\sum^{t-1}_{i=1}R_i^*1(A_i=a)}{\sum^{t-1}_{i=1}1(A_i=a)}, \quad a=L \quad or \quad R
$$
实则Sample-Average样本平均，后面平均值会无限地接近真实的期望值

策略函数？选择价值最大的那个行动，这一策略称之为greedy贪婪

但是根据贪婪策略在我们初始值为0的情况下，岂不是我们一开始选择哪一个就是哪一个了吗

于是我们可以让价值函数的初始值很大，比如998,于是我们选择哪一个，我们的Q都会变小，这就会使我们尝试其他行动，我们后面的Q值计算也要记入初始值，会鼓励更多的探索行为（reason：原本两个差距不大，计入初始很大的奖励后，期望低的那个就有可能超过高的那个，因此会探索更多原本期望低的）
$$
A_t=argmax_aQ_t(a)
$$


如果状态发生改变的话，这个方法就不可行了。

归根结底还是不能太贪婪，最有效的解决方法是$\epsilon-Greedy$



含义为在大多数情况下作出贪婪的选择，但是以一定的概率$\epsilon$，做出随机的选择

这里采用使$\epsilon=0.1$，初始值为$998$的强化学习方法

比如t=1时刻，我们随机选择左边，得到的奖励为526,于是Q（L）就变为（998+526）/2=762

在t=2时刻，右边的选择价值更高，选择右边，得到的奖励518，于是Q（R）变为（998+518）/2=758

在t=3时刻，左边的选择价值更高........

在某一时刻，假如现在左边的为656,右边为651,理应选择左边，但是由于$\epsilon-Greedy$方法，，随机选择了右边......

最后就会收敛到他们的均值

---

R将K arm Bandit的算法用python来实现，







